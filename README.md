# NLP Trigram Model - Text Prediction & Completion

A statistical Natural Language Processing system that predicts and completes sentences using a trigram language model trained on Persian text. (This project is also a part of the [Artificial Intelligence Course](https://github.com/zamirmehdi/Artificial-Intelligence-Course) repository.)

[![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)
[![NLP](https://img.shields.io/badge/NLP-Language%20Modeling-green.svg)](#)
[![Persian](https://img.shields.io/badge/Language-Persian-red.svg)](#)

<details> <summary><h2>ğŸ“š Table of Contents</h2></summary>

- [Project Overview](#-project-overview)
- [N-gram Language Models](#-n-gram-language-models)
  - [What is a Trigram?](#what-is-a-trigram)
  - [Example](#example)
  - [N-gram Types](#n-gram-types)
- [Project Structure](#ï¸-project-structure)
- [Files Description](#-files-description)
- [Requirements](#ï¸-requirements)
- [How to Run](#-how-to-run)
- [Input/Output](#-inputoutput)
- [Key Features](#-key-features)
- [Concepts Demonstrated](#-concepts-demonstrated)
- [Model Evaluation](#-model-evaluation)
- [Customization](#-customization)
- [Theoretical Background](#-theoretical-background)
- [Persian Language Considerations](#-persian-language-considerations)
- [References](#-references)
- [Project Information](#â„¹ï¸-project-information)
- [Related Projects](#-related-projects)
- [Contact](#-contact)

</details>

## ğŸ“‹ Project Overview

This project implements a **trigram language model** for Persian text that:
- Learns word patterns from a training corpus
- Predicts the next word given two previous words
- Completes partial sentences
- Generates coherent text based on learned probabilities

## ğŸ§  N-gram Language Models

### What is a Trigram?

A **trigram** is a sequence of three consecutive words. The trigram model predicts the next word based on the previous two words:

```
P(wâ‚ƒ | wâ‚, wâ‚‚) = Count(wâ‚, wâ‚‚, wâ‚ƒ) / Count(wâ‚, wâ‚‚)
```

### Example
Given: "Ù…Ù† Ø¨Ù‡" (I to/at)  
Predict: "Ù…Ø¯Ø±Ø³Ù‡" (school), "Ø®Ø§Ù†Ù‡" (home), "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡" (university), etc.

### N-gram Types

| Model | Context | Formula |
|-------|---------|---------|
| **Unigram** | No context | P(wáµ¢) |
| **Bigram** | 1 previous word | P(wáµ¢ \| wáµ¢â‚‹â‚) |
| **Trigram** | 2 previous words | P(wáµ¢ \| wáµ¢â‚‹â‚‚, wáµ¢â‚‹â‚) |
| **N-gram** | n-1 previous words | P(wáµ¢ \| wáµ¢â‚‹â‚™â‚Šâ‚, ..., wáµ¢â‚‹â‚) |

## ğŸ—‚ï¸ Project Structure

```
NLP-Project/
â”œâ”€â”€ main.py              # Trigram model implementation
â”œâ”€â”€ Train_data.txt       # Training corpus (Persian text)
â”œâ”€â”€ Test_data.txt        # Test cases
â”œâ”€â”€ labels.txt           # Expected outputs for evaluation
â””â”€â”€ Instruction NLP.pdf  # Project specification
```

## ğŸ“„ Files Description

### `main.py`
Core implementation containing:
- Text preprocessing (tokenization, normalization)
- Trigram probability calculation
- Model training
- Next word prediction
- Sentence completion
- Evaluation metrics

### `Train_data.txt`
Persian text corpus for training:
- Multiple sentences in Persian
- Used to build trigram frequency counts
- Vocabulary extraction

### `Test_data.txt`
Test cases for evaluation:
- Incomplete sentences
- Context pairs for prediction
- Evaluation scenarios

### `labels.txt`
Ground truth for testing:
- Expected predictions
- Correct completions
- Accuracy measurement

## âš™ï¸ Requirements

```bash
# Python 3.x
# Standard library only (no external dependencies)
```

### Optional (for enhanced features)
```bash
pip install hazm  # Persian text processing (if used)
```

## ğŸš€ How to Run

```bash
# Navigate to project directory
cd NLP-Project

# Run the trigram model
python main.py
```

### Program Flow
1. Load and preprocess training data
2. Build trigram frequency tables
3. Calculate probabilities
4. Load test cases
5. Make predictions
6. Compare with labels and calculate accuracy

## ğŸ“Š Input/Output

### Input
**Training Phase:**
- `Train_data.txt` - Persian sentences for model training

**Prediction Phase:**
- `Test_data.txt` - Contexts for prediction (two-word sequences)

### Output
- Predicted next words
- Probability scores
- Completed sentences
- Accuracy metrics
- Model statistics

### Example

**Input:** "Ù…Ù† Ø¨Ù‡"  
**Output:**
```
Top 3 predictions:
1. Ù…Ø¯Ø±Ø³Ù‡ (P = 0.35)
2. Ø®Ø§Ù†Ù‡ (P = 0.28)
3. Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ (P = 0.15)
```

## ğŸ¯ Key Features

### 1. Text Preprocessing
- Tokenization (word splitting)
- Normalization (character standardization)
- Handling Persian-specific characters
- Stop word consideration

### 2. Trigram Extraction
- Sliding window over text
- Frequency counting
- Context-word pair storage

### 3. Probability Calculation
```python
P(wâ‚ƒ | wâ‚, wâ‚‚) = Count(wâ‚, wâ‚‚, wâ‚ƒ) / Count(wâ‚, wâ‚‚)
```

### 4. Smoothing (Optional)
Handles zero-probability cases:
- Laplace smoothing (add-one)
- Add-k smoothing
- Back-off to bigram/unigram

### 5. Prediction Strategies
- **Greedy:** Select highest probability word
- **Sampling:** Random selection based on probability distribution
- **Beam search:** Multiple candidate sequences

## ğŸ“ Concepts Demonstrated

### Statistical NLP
- Language modeling
- Maximum likelihood estimation
- Markov assumption
- Probability distributions

### Text Processing
- Tokenization
- Normalization
- Vocabulary building
- Context windowing

### Machine Learning
- Training on corpus
- Model evaluation
- Accuracy metrics
- Overfitting considerations

### Algorithm Design
- Efficient data structures (dictionaries/hash tables)
- Probability calculations
- Text generation
- Performance optimization

## ğŸ“ˆ Model Evaluation

### Metrics
- **Accuracy:** Percentage of correct predictions
- **Perplexity:** Model's uncertainty (lower is better)
- **Top-K Accuracy:** Correct word in top K predictions

### Example Results
```
Test samples: 100
Correct predictions: 73
Accuracy: 73%
Average perplexity: 45.2
```

## ğŸ”§ Customization

You can modify:
- **N-gram size** (trigram â†’ bigram/4-gram)
- **Smoothing technique**
- **Training corpus** (add more data)
- **Prediction strategy** (greedy vs. sampling)

## ğŸ“– Theoretical Background

### Markov Assumption
The probability of a word depends only on a fixed number of previous words:
```
P(sentence) â‰ˆ âˆ P(wáµ¢ | wáµ¢â‚‹â‚‚, wáµ¢â‚‹â‚)
```

### Advantages
âœ… Simple and interpretable  
âœ… Fast training and prediction  
âœ… Works well for short contexts  
âœ… No complex neural architectures needed

### Limitations
âŒ Limited context window  
âŒ Data sparsity issues  
âŒ No semantic understanding  
âŒ Requires large corpus

## ğŸŒ Persian Language Considerations

### Challenges
- Right-to-left script
- Complex morphology
- Informal vs. formal variants
- Diacritics and vowel marks

### Solutions
- Proper Unicode handling
- Character normalization
- Consideration of language-specific features

## ğŸ“š References

- **Jurafsky, D., & Martin, J. H.** (2023). *Speech and Language Processing* (3rd ed.).
- **Manning, C. D., & SchÃ¼tze, H.** (1999). *Foundations of Statistical Natural Language Processing*.

## â„¹ï¸ Project Information

**Author:** Amirmehdi Zarrinnezhad  
**Course:** Artificial Intelligence  
**University:** Amirkabir University of Technology (Tehran Polytechnic) - Spring 2020  
**GitHub:** [AI_Final_Project-NLP](https://github.com/zamirmehdi/AI_Final_Project-NLP)

## ğŸ”— Related Projects

This project is part of the [Artificial Intelligence Course](https://github.com/zamirmehdi/Artificial-Intelligence-Course) repository.

**Other AI Projects:**
- [Students Lineup](https://github.com/zamirmehdi/Artificial-Intelligence-Course/tree/main/Students-Lineup-Project) - Search algorithms
- [Super Mario LRTA*](https://github.com/zamirmehdi/AI-Project-Super-Mario) - Heuristic pathfinding


## ğŸ“§ Contact  
Questions or collaborations? Feel free to reach out!

**ğŸ“§ Email:** amzarrinnezhad@gmail.com  
**ğŸŒ GitHub:** [@zamirmehdi](https://github.com/zamirmehdi)


---

<div align="center">

[â¬† Back to Main Repository](https://github.com/zamirmehdi/Artificial-Intelligence-Course)

<p align="right">(<a href="#top">back to top</a>)</p>
</div>



<div align="center">

â­ **If you found this project helpful, please consider giving it a star!** â­

*Amirmehdi Zarrinnezhad*

</div>
